{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Gaussian Nodes with RPCircuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RPCircuits, Random, Distributions\n",
    "\n",
    "Random.seed!(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a gaussian with `mean = 0.3` and `variance = 1.0` using the `Distributions` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal{Float64}(μ=0.3, σ=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.3, 1.0\n",
    "gauss = Distributions.Normal(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate a dataset `D` with `N` samples of the previous distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000×1 Matrix{Float64}:\n",
       "  1.0883556016042917\n",
       " -0.5798585959543994\n",
       " -0.573793482209626\n",
       " -0.4332549644348927\n",
       " -0.40219519875768045\n",
       "  0.22741813195413008\n",
       " -0.6129233863399266\n",
       "  0.9316208311167526\n",
       "  1.7386832757114135\n",
       " -0.05416984337044606\n",
       "  1.096126919278033\n",
       " -0.8753711332175869\n",
       " -0.2933950393067663\n",
       "  ⋮\n",
       "  1.6653626910141202\n",
       "  0.38197143859308064\n",
       " -0.10460459321822108\n",
       " -0.22081326086068603\n",
       "  1.4174663272233832\n",
       "  2.6213309220930725\n",
       " -0.9800280316976422\n",
       "  0.3459752706815864\n",
       "  0.8836106784973501\n",
       " -0.860814000747588\n",
       " -0.04220185450855518\n",
       "  0.15538227617934502"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100_000\n",
    "\n",
    "samples = Distributions.rand(gauss, N)\n",
    "\n",
    "D = reshape(samples, length(samples), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `RPCircuits`, we create a `Gaussian Node` `G` with the same `mean` and `variance` as the previous gaussian distribution. Then, we apply the `NLL` function to see the Negative Log-Likelihood of `G` w.r.t. `D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model NLL = 1.4212436987533956\n"
     ]
    }
   ],
   "source": [
    "G = RPCircuits.Gaussian(1, mean, var)\n",
    "\n",
    "println(\"Original model NLL = \", NLL(G, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an arbitraty `Gaussian Node`that has both `mean` and `variance` different from the distribution `gauss`. Then, we apply the `EM` algorithm to learn a better distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM initial NLL = 1.6179711843349411\n",
      "EM final NLL = 1.4212340080384096\n",
      "G_em = gaussian 1 0.2970289942983956 1.0046015042225562\n"
     ]
    }
   ],
   "source": [
    "G_em = RPCircuits.Gaussian(1, -0.15, 2.5)\n",
    "\n",
    "L_em = SEM(G_em; gauss=true)\n",
    "\n",
    "println(\"EM initial NLL = \", NLL(G_em, D))\n",
    "\n",
    "for i = 1:50\n",
    "    update(L_em, D; learngaussians=true, verbose=false)\n",
    "end\n",
    "\n",
    "println(\"EM final NLL = \", NLL(G_em, D))\n",
    "\n",
    "println(\"G_em = $G_em\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the example above, we create a `Gaussian Node` with both `mean` and `variance` differente from the distribution `gauss`. However, we apply the `Gradient Descent` algorithm in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad initial NLL = 1.6179711843349411\n",
      "Grad final NLL = 1.4536896433734792\n",
      "G_grad = gaussian 1 0.29543688549190184 1.4738744413443041\n"
     ]
    }
   ],
   "source": [
    "G_grad = RPCircuits.Gaussian(1, -0.15, 2.5)\n",
    "\n",
    "L_grad = GRAD(G_grad, gauss=true)\n",
    "\n",
    "println(\"Grad initial NLL = \", NLL(G_grad, D))\n",
    "\n",
    "for i = 1:2_500\n",
    "    update(L_grad, D; learningrate=0.01, learngaussians=true, verbose=false)\n",
    "end\n",
    "\n",
    "println(\"Grad final NLL = \", NLL(G_grad, D))\n",
    "println(\"G_grad = $G_grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
